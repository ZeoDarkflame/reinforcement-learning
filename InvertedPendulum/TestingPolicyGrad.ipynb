{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functions\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.distributions.normal import Normal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ActorNet,self).__init__()\n",
    "        self.fc1 = nn.Linear(3,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,2)    # output is std dev and mean\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = functions.relu(self.fc1(x))\n",
    "        x = functions.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x[:,1] = torch.clamp(x[:,1].clone(),min=0.1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    " \n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self,tup_le):\n",
    "        self.memory.append(tup_le)\n",
    "        \n",
    "    def sample(self,sample_size):\n",
    "        try:\n",
    "            ob, action, reward, newob, done = zip(*random.sample(self.memory,sample_size))\n",
    "        except:\n",
    "            ob, action, reward, newob, done = zip(*random.sample(self.memory,len(self.memory)))\n",
    "        \n",
    "        return ob,action,reward,newob,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self,env,Actor,Actor_optimizer,Critic,fixed_net_Critic,Critic_optimizer,buffercap,gamma = 0.99):\n",
    "        #Critic\n",
    "        self.Critic = Critic\n",
    "        self.fixed_net_Critic = fixed_net_Critic\n",
    "        self.Critic_optimizer = Critic_optimizer\n",
    "\n",
    "        #Actor\n",
    "        self.Actor = Actor\n",
    "        self.Actor_optimizer = Actor_optimizer\n",
    "\n",
    "        #Replay Memory and ENV\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffercap)\n",
    "        self.env = env\n",
    "        \n",
    "    \n",
    "    def gen_episode(self,render = False,not_training = False):#generate one episode\n",
    "\n",
    "        ob = self.env.reset()\n",
    "        done = False\n",
    "        reward_count = 0\n",
    "        if(render):\n",
    "            env.render()\n",
    "            time.sleep(2)\n",
    "        \n",
    "        while(not done):\n",
    "\n",
    "            ## get action\n",
    "            if(render):\n",
    "                env.render()\n",
    "                time.sleep(0.15)\n",
    "            with torch.no_grad():\n",
    "                out = self.Actor(torch.from_numpy(ob.reshape(1,-1)).float().to(device))\n",
    "                try:\n",
    "                  dist = Normal(out[0,0],out[0,1])\n",
    "                except:\n",
    "                  print(out[0,0],out[0,1])\n",
    "                  dist = Normal(out[0,0],out[0,1])\n",
    "                action = dist.sample().cpu().numpy().reshape(1)   # because environment requires output in shape of 1\n",
    "\n",
    "            \n",
    "            newob, rew, done, _ = self.env.step(action)\n",
    "            self.buffer.push((ob,action,rew,newob,done))\n",
    "            if(not not_training):\n",
    "                self.train_actor((ob,action,rew,newob,done))\n",
    "            ob = newob\n",
    "            reward_count += rew\n",
    "            if(not not_training):\n",
    "                self.train_critic()\n",
    "        \n",
    "        return reward_count/200\n",
    "    \n",
    "\n",
    "    def train_critic(self):   #perform one step of gradient descent\n",
    "        batch_size = 64\n",
    "        X,actions,rewards,Y,terminated = self.buffer.sample(batch_size)\n",
    "        #print('Type of X is: ',type(X))\n",
    "\n",
    "        X = np.array(X)\n",
    "        #print('Shape of X is: ',X.shape)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Y = self.fixed_net_Critic(torch.from_numpy(Y).float().to(device)).cpu().numpy()\n",
    "        #print(Y.shape)\n",
    "        rewards = np.array(rewards)\n",
    "        rewards = rewards.reshape(rewards.shape[0],1)\n",
    "        terminated = np.array(terminated)\n",
    "        terminated = terminated.reshape(terminated.shape[0],1)\n",
    "        #print(1-terminated)\n",
    "        Y = rewards + self.gamma*Y*(1-terminated)\n",
    "\n",
    "        \n",
    "        loss = nn.MSELoss()\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        Y = torch.from_numpy(Y).float().to(device)\n",
    "        self.Critic_optimizer.zero_grad()\n",
    "        output = self.Critic(X)\n",
    "        cost = loss(output,Y)\n",
    "        cost.backward(retain_graph=True)  #remove retain graph to get the error about the intermediate values being freed # add the retain graph line if you train the actor here\n",
    "        self.Critic_optimizer.step()\n",
    "\n",
    "        #start training actor\n",
    "        #with torch.no_grad():\n",
    "        #  adv = torch.from_numpy(rewards).float().to(device) + self.gamma*Y - output\n",
    "        #self.Actor_optimizer.zero_grad()\n",
    "        #actors_output = self.Actor(X)\n",
    "        #loss = -1*torch.mean(torch.log(actors_output)*adv)\n",
    "        #loss.backward()\n",
    "        #self.Actor_optimizer.step()\n",
    "        \n",
    "\n",
    "    def train_actor(self,tup_le):\n",
    "        states,actions,rewards,state_dashes,terminated = tup_le\n",
    "        states = np.array(states)\n",
    "        states = states.reshape(1,-1)\n",
    "        state_dashes = np.array(state_dashes)\n",
    "        state_dashes = state_dashes.reshape(1,-1)\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        actions = np.array(actions)\n",
    "        #print(type(actions),actions.shape)\n",
    "        #print('Rewards shape is: ',rewards.shape)\n",
    "        #print('Terminated shape is: ',terminated.shape)\n",
    "\n",
    "        #define advantage function\n",
    "        with torch.no_grad():\n",
    "            V_s = self.Critic(torch.from_numpy(states).float().to(device))\n",
    "            V_s_dash = self.Critic(torch.from_numpy(state_dashes).float().to(device))\n",
    "        adv = torch.from_numpy(rewards).float().to(device) + self.gamma*V_s_dash - V_s\n",
    "        \n",
    "        self.Actor_optimizer.zero_grad()\n",
    "        output = self.Actor(torch.from_numpy(states).float().to(device))\n",
    "        #print('Output size is: ',output.size())\n",
    "        dist = Normal(output[0,0],output[0,1])\n",
    "        \n",
    "        #print('Output is: ',output)\n",
    "        loss = -1*dist.log_prob(torch.from_numpy(actions).to(device))*adv\n",
    "        #print('Loss is: ',loss)\n",
    "        loss.backward()\n",
    "        self.Actor_optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    print('CUDA Available')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorNet(\n",
       "  (fc1): Linear(in_features=3, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Actor = ActorNet()\n",
    "Actor.load_state_dict(torch.load('InvPenduActor2.pth'))\n",
    "Actor.eval()\n",
    "Actor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Pendulum-v0'\n",
    "env = gym.make(name)\n",
    "agent = Agent(env,Actor,None,None,None,None,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.4405231945576675\n"
     ]
    }
   ],
   "source": [
    "checkward = agent.gen_episode(render = True,not_training=True)\n",
    "print(checkward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
